name: sac

agent_name: ${now:%Y%m%d}_sac_gnn  # Agent name for training or evaluation (default: today's date + '_sac_gnn')

map_path: "../example_problems/custom_warehouse.domain/warehouse_8x6.json"  # Path to json file with map, agents, tasks, etc.

use_dummy_goals_for_idle_agents: false  # Use dummy goals for idle agents in PIBT planner

cplexpath: "None"  # Defines directory of the CPLEX installation
  
directory: "saved_files"  # Defines directory where to save files

net: "TransformerConv"

max_episodes: 10000  # Number of episodes to train agent (default: 16k)

max_steps: 150  # Number of steps per episode

no_cuda: false  # Disables CUDA training

batch_size: 100  # Defines batch size

p_lr: 1e-3  # Define policy learning rate

q_lr: 1e-3  # Defines q-value learning rate

alpha: 0.3  # Defines entropy coefficient

auto_entropy: false  # Use automatic entropy tuning

hidden_size: 256  # Defines hidden units in the MLP layers

clip: 500  # Clip value for gradient clipping (default: 500)

checkpoint_path: "FIXED_ROADS"  # Path where to save model checkpoints

train: true # Switches updating the weights on and off

start_training_at_episode: 10

visu_episode_list: [0, 5, 10, 12, 15, 20, 30, 50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000]

load_from_ckeckpoint: false  # Load model from checkpoint

load_external_actor: false  # Load externally trainged actor e.g. from imitation learning in colab

external_actor_path: "../nn_optimization/actors/actor_20percent_Transformer.pth"  # Path to externally trained actor

deterministic_actor: false  # Use deterministic actor during training

input_size: 14 # Number of node features (15 + 20 (distance_until_agent_avail_MAX))

only_q_steps: 0 

wandb: true # Enables Weights and Biases logging

tensorboard: false # Enables Tensorboard logging

normalise_obs: true # Normalise observations

edge_feature_dim: 7 # Number of edge features (6 + 10 (distance_until_agent_avail_MAX))



# Reward weigths
rew_w_task_finish: 0
rew_w_final_tasks_finished: 0
rew_w_roadmap_progress: 20