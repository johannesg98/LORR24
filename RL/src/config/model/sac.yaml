name: sac

agent_name: ${now:%Y%m%d}_sac_gnn  # Agent name for training or evaluation (default: today's date + '_sac_gnn')

map_path: "../example_problems/custom_warehouse.domain/warehouse_8x6.json"  # Path to json file with map, agents, tasks, etc.

use_dummy_goals_for_idle_agents: false  # Use dummy goals for idle agents in PIBT planner

cplexpath: "None"  # Defines directory of the CPLEX installation
  
directory: "saved_files"  # Defines directory where to save files

net: "TransformerConvAction"

scheduler_type: "RL"

allow_task_change: false  # Allow agents to change tasks while they didnt arrive at the first errand yet

allow_task_change_only_during_testing: false

max_episodes: 10000  # Number of episodes to train agent (default: 16k)

max_steps: 150  # Number of steps per episode

no_cuda: false  # Disables CUDA training

batch_size: 100  # Defines batch size

p_lr: 1e-3  # Define policy learning rate

q_lr: 1e-3  # Defines q-value learning rate

alpha: 0.3  # Defines entropy coefficient

auto_entropy: false  # Use automatic entropy tuning

hidden_size: 256  # Defines hidden units in the MLP layers

clip: 500  # Clip value for gradient clipping (default: 500)

checkpoint_path: "GPU300_TransformerConvAction_Err50_bcktr20_Fin50_tryidk"  # Path where to save model checkpoints

train: true # Switches updating the weights on and off

start_training_at_episode: 10

skip_actor: false # Use skip_actor-ILP instead of the actor

visu_episode_list: [0, 5, 10, 12, 15, 20, 30, 50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000]

load_from_ckeckpoint: false  # Load model from checkpoint

load_external_actor: false  # Load externally trainged actor e.g. from imitation learning in colab

external_actor_path: "../nn_optimization/actors/actor_20percent_Transformer.pth"  # Path to externally trained actor

deterministic_actor: false  # Use deterministic actor

input_size: 17 # Number of node features (15 + 20 (distance_until_agent_avail_MAX))

test_episodes: 10 # Number of episodes to test agent

determinisitc_backup: False #use q-value deterministic backup

pretrained_path: null 

only_q_steps: 0 

wandb: true # Enables Weights and Biases logging

tensorboard: false # Enables Tensorboard logging

mask_impactless_actions: true # Mask useless actions

backtrack_reward: true # Use actual traveldistance and backtrack to initial action

backtrack_reward_type: "MaxDist-Time" # Type of backtrack reward. Options: "MaxDist-Time", "DividedTime", "SquaredNormalized", "OnlyTime"

use_markovian_new_obs: false # Do not use the current steps new obs but wait until next agents become available and use that obs instead

normalise_obs: true # Normalise observations

message_passing_edge_limit: 28 # Maximum distance of edges for message passing and edge atributes

use_message_passing: true # Use message passing

edge_feature_dim: 6 # Number of edge features (6 + 10 (distance_until_agent_avail_MAX))

distance_until_agent_avail_MAX: 20 # Max distance where agent tells node when it becomes available after finishing the task(observation)

only_consider_first_n_steps_for_training: false         # Only consider first n env-steps for training. Can be either False or an integer value (number of steps).




# Reward weigths
rew_w_Astar: 0    # best: 20
rew_w_dist: 0
rew_w_idle: 0     # best: 20
rew_w_task_finish: 50
rew_w_backtrack: 20 # best: 20
rew_w_immitation: 0
rew_w_final_tasks_finished: 0
rew_w_first_errand: 50
rew_w_change_task_backtrack: 0 



# Slope rewards: adjust rewards over training time. Starts at normal weights above
slope_rewards: false

slope_backtrack_start_t: 1000
slope_backtrack_end_t: 2000
slope_backtrack_final_w: 0

slope_first_errand_start_t: 1000
slope_first_errand_end_t: 2000
slope_first_errand_final_w: 0



# lr_rate_scheduling
lr_rate_scheduling: false

lr_rate_scheduling_start_t: 1000
lr_rate_scheduling_end_t: 2000
lr_rate_scheduling_final_p_lr: 1e-4
lr_rate_scheduling_final_q_lr: 1e-4