name: sac

agent_name: ${now:%Y%m%d}_sac_gnn  # Agent name for training or evaluation (default: today's date + '_sac_gnn')

map_path: "../example_problems/custom_warehouse.domain/warehouse_8x6.json"  # Path to json file with map, agents, tasks, etc.

cplexpath: "None"  # Defines directory of the CPLEX installation
  
directory: "saved_files"  # Defines directory where to save files

max_episodes: 10000  # Number of episodes to train agent (default: 16k)

max_steps: 150  # Number of steps per episode

no_cuda: false  # Disables CUDA training

batch_size: 100  # Defines batch size

p_lr: 1e-3  # Define policy learning rate

q_lr: 1e-3  # Defines q-value learning rate

alpha: 0.3  # Defines entropy coefficient

auto_entropy: false  # Use automatic entropy tuning

hidden_size: 256  # Defines hidden units in the MLP layers

clip: 500  # Clip value for gradient clipping (default: 500)

checkpoint_path: "nn_test_actor_37percent"  # Path where to save model checkpoints

train: false # Switches updating the weights on and off

skip_actor: false # Use skip_actor-ILP instead of the actor

visu_episode_list: [0, 5, 10, 12, 15, 20, 30, 50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000]

load_from_ckeckpoint: false  # Load model from checkpoint

load_external_actor: true  # Load externally trainged actor e.g. from imitation learning in colab

external_actor_path: "../nn_optimization/actors/actor_37percent.pth"  # Path to externally trained actor

use_LSTM: false  # Use LSTM in the model

input_size: 3 # Number of node features

test_episodes: 10 # Number of episodes to test agent

determinisitc_backup: False #use q-value deterministic backup

pretrained_path: null 

only_q_steps: 0 

wandb: true # Enables Weights and Biases logging

tensorboard: false # Enables Tensorboard logging

mask_impactless_actions: true # Mask useless actions

backtrack_reward: true # Use actual traveldistance and backtrack to initial action

use_markovian_new_obs: false # Do not use the current steps new obs but wait until next agents become available and use that obs instead

normalise_obs: true # Normalise observations




# Reward weigths
rew_w_Astar: 0    # best: 20
rew_w_idle: 0     # best: 20
rew_w_task_finish: 0
rew_w_backtrack: 20 # best: 20
rew_w_immitation: 0