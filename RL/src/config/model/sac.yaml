# Intro
name: sac
agent_name: ${now:%Y%m%d}_sac_gnn  # Agent name for training or evaluation (default: today's date + '_sac_gnn')
cplexpath: "None"  # Defines directory of the CPLEX installation (default: "None")
directory: "saved_files"  # Defines directory where to save files
no_cuda: false  # Disables CUDA training


# Environment
map_path: "../example_problems/custom_warehouse.domain/warehouse_8x6.json"  # Path to json file with map, agents, tasks, etc., options: warehouse_6x4, warehouse_8x6, warehouse_9x8, warehouse_13x12 (default: "../example_problems/custom_warehouse.domain/warehouse_8x6.json")
use_dummy_goals_for_idle_agents: false  # Use dummy goals for idle agents in PIBT planner, 0 = goal is current postion (-> wait), 1 = goal is spawn position, 2 = goal is random position (default: 0)
scheduler_type: "RL"  # Scheduler type, options: RL, NoManSky, default, ILP (default: "RL")
time_per_step: 70 # time in ms that the task-scheduler and path-planner have. 1000 in LRR. For small maps ~70 is enough and more time doesn't improve the performance. (default: 70)


# RL algorithm parameters
batch_size: 100  # Defines batch size (default: 100)
p_lr: 1e-3  # Define policy learning rate (default: 1e-3)
q_lr: 1e-3  # Defines q-value learning rate (default: 1e-3)
alpha: 0.3  # Defines entropy coefficient (default: 0.3)
auto_entropy: false  # Use automatic entropy tuning (default: false)
clip: 500  # Clip value for gradient clipping (default: 500)


# Training
train: false # Switches updating the weights on and off (default: true)
max_episodes: 4000  # Number of episodes to train agent (default: 10k)
max_steps: 150  # Number of steps per episode (default: 150)
start_training_at_episode: 10 # collects samples for the first x episodes and only then starts training (default: 10)
only_q_steps: 0 # Only use Q-learning for x episodes (default: 0)
skip_actor: false # Use skip_actor-ILP instead of the actor (default: false)
mask_impactless_actions: true # Mask actions where no free agents are available and action therefore has no impact on performance (default: true)
use_markovian_new_obs: false # Do not use the current step's new_obs but wait until next agents become available and use that obs instead (default: false)
only_consider_first_n_steps_for_training: false         # Only consider first n env-steps for training. Can be either False or an integer value (number of steps) (default: false)
allow_task_change: false  # Allow agents to change tasks while they didnt arrive at the first errand yet (default for training: false)
allow_task_change_only_during_testing: false  # Allows task changes during testing while training



# Testing
# -> run via seperate test script "test.py": Shares most parameters with training. Gets ignored when executing train.py
load_test_checkpoint_path: "../example_checkpoints/warehouse_8x6_200_agents"  # Path to trained example checkpoints, options: warehouse_6x4_100_agents, warehouse_8x6_200_agents, warehouse_9x8_300_agents, warehouse_13x12_500_agents
test_episodes: 1  # Number of episodes to test agent. Results get averaged.
test_max_steps: 10000   # Number of environment steps. We use 150 for training, here for testing we use up to 10,000 (default: 10000)
test_time_per_step: 70 # time in ms that the task-scheduler and path-planner have. 1000 are given in the LRR. For small maps ~70 is enough and more time doesn't improve the performance. For fair comparisson 1000. (default: 70)
test_allow_task_change: true  # Allow agents to change tasks while they didnt arrive at the first errand yet (default for testing: true)


# Actor and Critic networks
net: "TransformerConvAction"   # Network architecture, options: "TransformerConv", "TransformerConvAction", "GCNConv", "GCNConvPenta", "GATConv", "NNConv", (default: "TransformerConvAction")
hidden_size: 256  # Defines hidden units in the MLP layers (default: 256)
input_size: 17 # Number of node features (see LRRParser) (default: 17)
use_message_passing: true # Use message passing graph instead of main graph
message_passing_edge_limit: 28 # Nodes in the message passing graph are connected by an edge if they are closer than this distance
edge_feature_dim: 6 # Number of edge features (default: 6)
deterministic_actor: false  # Use deterministic actor instead of stochastic (Dirichlet) (default: false)


# Observations
distance_until_agent_avail_MAX: 20 # Max distance where agent tells node when it becomes available after finishing the task(observation)
normalise_obs: true # Normalise observations


# Tracking of training progress
wandb: false # Enables Weights and Biases logging
tensorboard: false # Enables Tensorboard logging
visu_episode_list: [0, 5, 10, 12, 15, 20, 30, 50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000]   # saves the visualization of these episodes


# Checkpoints
checkpoint_path: "GPU_TransformerConvAction_F50_B20_Fin50_warehouse8x6"  # Path where to save model checkpoints or load them from
load_from_ckeckpoint: false  # Load model from checkpoint (default: false)
load_external_actor: false  # Load externally trained actor e.g. from imitation learning in colab
external_actor_path: "../nn_optimization/actors/actor_20percent_Transformer.pth"  # Path to externally trained actor


# Reward (default for best performing RL: rew_w_task_finish = 50, rew_w_backtrack = 20, rew_w_first_errand = 50, rest = 0)
rew_w_Astar: 0    # best: 20
rew_w_dist: 0
rew_w_idle: 0     # best: 20
rew_w_task_finish: 50
rew_w_backtrack: 20 # best: 20
rew_w_immitation: 0
rew_w_final_tasks_finished: 0
rew_w_first_errand: 50
rew_w_change_task_backtrack: 0 
backtrack_reward: true # Use actual traveldistance and backtrack to initial action
backtrack_reward_type: "MaxDist-Time" # Type of backtrack reward. Options: "MaxDist-Time", "DividedTime", "SquaredNormalized", "OnlyTime" (default: "MaxDist-Time")


# Slope rewards: adjust rewards over training time. Starts at normal weights above
slope_rewards: false    # activates slope rewards (default: false)
slope_backtrack_start_t: 1000
slope_backtrack_end_t: 2000
slope_backtrack_final_w: 0
slope_first_errand_start_t: 1000
slope_first_errand_end_t: 2000
slope_first_errand_final_w: 0


# lr_rate_scheduling
lr_rate_scheduling: false   # activates learning rate scheduling (default: false)
lr_rate_scheduling_start_t: 1000
lr_rate_scheduling_end_t: 2000
lr_rate_scheduling_final_p_lr: 1e-4
lr_rate_scheduling_final_q_lr: 1e-4