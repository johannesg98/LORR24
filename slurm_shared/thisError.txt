wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: johannes-gaber (johannesg98) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.7
wandb: Run data is saved locally in /mnt/home/jgaber/LORR24_johannesg98/wandb/run-20250520_230045-99uenoh2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run GPU_TransformerConv_backtrack20_NoDummy
wandb: ‚≠êÔ∏è View project at https://wandb.ai/johannesg98/warehouse_8x6_ag200
wandb: üöÄ View run at https://wandb.ai/johannesg98/warehouse_8x6_ag200/runs/99uenoh2
  0%|          | 0/10000 [00:00<?, ?it/s]Episode 1 | Reward: 325760.00 | NumTasksFinishe: 198.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 0/10000 [00:13<?, ?it/s]/mnt/home/jgaber/miniconda3/envs/myenv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/mnt/home/jgaber/miniconda3/envs/myenv/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
Episode 1 | Reward: 325760.00 | NumTasksFinishe: 198.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 1/10000 [00:13<38:48:02, 13.97s/it]Episode 2 | Reward: 340620.00 | NumTasksFinishe: 241.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 1/10000 [00:28<38:48:02, 13.97s/it]Episode 2 | Reward: 340620.00 | NumTasksFinishe: 241.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 2/10000 [00:28<39:24:36, 14.19s/it]Episode 3 | Reward: 335600.00 | NumTasksFinishe: 220.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 2/10000 [00:42<39:24:36, 14.19s/it]Episode 3 | Reward: 335600.00 | NumTasksFinishe: 220.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 3/10000 [00:42<39:17:28, 14.15s/it]Episode 4 | Reward: 335040.00 | NumTasksFinishe: 228.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 3/10000 [00:57<39:17:28, 14.15s/it]Episode 4 | Reward: 335040.00 | NumTasksFinishe: 228.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 4/10000 [00:57<39:52:51, 14.36s/it]Episode 5 | Reward: 376120.00 | NumTasksFinishe: 255.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 4/10000 [01:11<39:52:51, 14.36s/it]Episode 5 | Reward: 376120.00 | NumTasksFinishe: 255.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 5/10000 [01:11<40:22:11, 14.54s/it]Episode 6 | Reward: 329800.00 | NumTasksFinishe: 206.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 5/10000 [01:25<40:22:11, 14.54s/it]Episode 6 | Reward: 329800.00 | NumTasksFinishe: 206.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 6/10000 [01:43<56:26:34, 20.33s/it]Episode 7 | Reward: 353880.00 | NumTasksFinishe: 255.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 6/10000 [01:58<56:26:34, 20.33s/it]Episode 7 | Reward: 353880.00 | NumTasksFinishe: 255.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 7/10000 [01:58<51:15:44, 18.47s/it]Episode 8 | Reward: 364980.00 | NumTasksFinishe: 237.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 7/10000 [02:12<51:15:44, 18.47s/it]Episode 8 | Reward: 364980.00 | NumTasksFinishe: 237.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 8/10000 [02:12<47:46:47, 17.21s/it]Episode 9 | Reward: 337460.00 | NumTasksFinishe: 232.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 8/10000 [02:27<47:46:47, 17.21s/it]Episode 9 | Reward: 337460.00 | NumTasksFinishe: 232.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 9/10000 [02:27<45:22:52, 16.35s/it]Episode 10 | Reward: 366560.00 | NumTasksFinishe: 251.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 9/10000 [02:41<45:22:52, 16.35s/it]Episode 10 | Reward: 366560.00 | NumTasksFinishe: 251.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 10/10000 [02:41<43:55:40, 15.83s/it]Episode 11 | Reward: 327500.00 | NumTasksFinishe: 225.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 10/10000 [02:56<43:55:40, 15.83s/it]Episode 11 | Reward: 327500.00 | NumTasksFinishe: 225.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 11/10000 [02:56<43:04:37, 15.52s/it]Episode 12 | Reward: 327840.00 | NumTasksFinishe: 242.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 11/10000 [03:18<43:04:37, 15.52s/it]Episode 12 | Reward: 327840.00 | NumTasksFinishe: 242.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 12/10000 [03:18<48:26:23, 17.46s/it]Episode 13 | Reward: 357040.00 | NumTasksFinishe: 243.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 12/10000 [03:40<48:26:23, 17.46s/it]Episode 13 | Reward: 357040.00 | NumTasksFinishe: 243.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 13/10000 [03:40<52:20:20, 18.87s/it]Episode 13 | Reward: 357040.00 | NumTasksFinishe: 243.0 | Checkpoint: GPU_TransformerConv_backtrack20_NoDummy:   0%|          | 13/10000 [03:42<47:31:53, 17.13s/it]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/mnt/home/jgaber/LORR24_johannesg98/RL/train.py", line 93, in main
    model.learn(cfg) #online RL
    ^^^^^^^^^^^^^^^^
  File "/mnt/home/jgaber/LORR24_johannesg98/RL/src/algos/sac.py", line 722, in learn
    batch = self.replay_buffer.sample_batch(cfg.model.batch_size)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/jgaber/LORR24_johannesg98/RL/src/algos/sac.py", line 131, in sample_batch
    batch = Batch.from_data_list(data, follow_batch=['x_s', 'x_t']).to(self.device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/jgaber/miniconda3/envs/myenv/lib/python3.12/site-packages/torch_geometric/data/batch.py", line 97, in from_data_list
    batch, slice_dict, inc_dict = collate(
                                  ^^^^^^^^
  File "/mnt/home/jgaber/miniconda3/envs/myenv/lib/python3.12/site-packages/torch_geometric/data/collate.py", line 109, in collate
    value, slices, incs = _collate(attr, values, data_list, stores,
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/jgaber/miniconda3/envs/myenv/lib/python3.12/site-packages/torch_geometric/data/collate.py", line 204, in _collate
    value = torch.cat(values, dim=cat_dim or 0, out=out)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
